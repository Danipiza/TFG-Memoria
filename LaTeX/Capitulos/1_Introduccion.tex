\chapter{Introducción}

En este capítulo se presenta una perspectiva general del contexto en el que se ha llevado a cabo el proyecto. Además de las dificultades encontradas durante su desarrollo para alcanzar las contribuciones mencionadas, se detallan cada uno de los propósitos perseguidos en él.


\section{Definición y alcance del proyecto}

	El desarrollo de las Inteligencias Artificiales en nuestra sociedad ha sido un fenómeno de gran relevancia, además de popular, en los últimos años. Estas tecnologías han llegado para quedarse y están mejorando nuestra calidad de vida. Desde la automatización de tareas hasta la asistencia virtual\cite{lugano2017virtual}, estas IAs desempeñan un papel cada vez más importante en nuestro día a día.
	Con el advenimiento del Internet de alta velocidad y la proliferación de datos, las empresas tecnológicas se enfrentan a la necesidad creciente de desarrollar servicios de alta calidad en un mercado muy competitivo. Actualmente, se invierte mucho dinero y tiempo en mejorar y diseñar algoritmos para implementar Inteligencias Artificiales para el acceso público\cite{thomas2021global}.
	
	El entrenamiento y ejecución de estos algoritmos para modelar inteligencias artificiales consumen mucha energía, además de provocar una cantidad excesiva de emisiones de CO\(_2\). La empresa tecnológica \textit{Hugging Face}, desarrolladora de \textit{BLOOM}\cite{BloomAI}, la primera LLM (Large Language Model) multilenguaje entrenada de forma transparente, tuvo la colaboración de muchos investigadores. Este proyecto, con \textit{176} mil millones de parámetros, es capaz de generar texto en \textit{46} idiomas y \textit{13} lenguajes de programación. No obstante, estimaron que el entrenamiento de esta inteligencia artificial emitió \textit{25} toneladas de CO\(_2\), cifra que se duplicó al contar el coste de producción del equipo informático usado\cite{kirkpatrick2023carbon}. Los investigadores se están enfocando en evaluar y reducir el impacto ambiental de las tecnologías de IA. Una prueba de ello es el desarrollo de \textit{CarbonTracker}\cite{jeppesen2021carbon} (CTI), un equipo de especialistas financieros que asumen el riesgo climático como realidad de los mercados financieros actuales. El objetivo de esta herramienta es predecir y reducir la huella de carbono de las etapas de entrenamiento de los modelos de IA\cite{mor2021artificial}.
	
	El uso de la programación distribuida, más específicamente, aplicaciones basadas en MPI, permite ejecutar varios procesos en paralelo, dividiendo la carga de trabajo y reduciendo el tiempo de ejecución. Al contrario de los programas basados en el modelo de memoria compartida, en el cual se pueden dar problemas de sincronización, cada proceso generado tiene su propia memoria local, evadiendo estos problemas. Sin embargo, hay que diseñar implementaciones correctas y eficientes para no tener más complejidad espacial (uso de memoria) de la esperada. Las conexiones entre los procesos se pueden configurar para maximizar la eficiencia y reducir el tiempo de cómputo. Una de las más populares es el modelo \textit{Master-Worker}. El proceso \textit{master} se encarga de distribuir el trabajo a los procesos \textit{workers} para que, en paralelo, puedan ejecutar la misma tarea con conjuntos de datos más reducidos. Al finalizar la tarea, cada \textit{worker} envía sus datos procesados, y si el proceso \textit{master} no ha terminado la ejcución, espera para recibir más datos hasta procesar completamente el data-set inicial. MPI permite la comunicación eficiente entre procesos, mejorando la escalabilidad y reduciendo el tiempo de procesamiento. Esta metodología, además de mejorar el rendimiento, también simplifica la gestión de recursos, mejorando la utilización del hardware disponible en el sistema. 
	
	Los algoritmos de IA suelen manejar un vasto número de datos para entrenar y evaluar los modelos deseados. Por eso es fundamental diseñar estrategias para distribuir los datos y dividir las cargas de trabajo de manera equitativa, controlando el flujo de datos para evitar cuellos de botella (un proceso consume más tiempo de ejecución que los demás, quedando el resto en espera), y equilibrar los recursos disponibles. Esto incluye, además de la optimización del tiempo de ejecución, una gestión efectiva de la memoria y una reducción en la latencia (tiempo de espera en transmitir los paquetes de información en una red) en la comunicación entre los procesos. 
	
	En este proyecto se diseñarán e implementarán varias estrategias para diferentes algoritmos de IA, con el objetivo de reducir el tiempo de ejecución.





\section{Motivación}
	Actualmente hay muchas implementaciones de algoritmos de IA. \textit{Scikit learn} es una biblioteca de \textit{Python} adecuada para probar cualquier técnica. Esta biblioteca, como la mayoría, ejecuta los algoritmos de manera secuencial, sin dividir la carga de trabajo. Las implementaciones están estudiadas y perfeccionadas para realizar los cálculos en un único proceso, pero se puede reducir el tiempo de ejecución aplicando paralelismo. 
	
	Las arquitecturas distribuidas, junto con las técnicas de cómputo de alto rendimiento (HPC, por sus siglas en inglés), son una de las soluciones más apropiadas para los usuarios finales: científicos y empresas. La búsqueda de un rendimiento óptimo, enfocada a sistemas altamente distribuidos, demandan enfoques adaptables y escalables para implementar aplicaciones científicas de alto rendimiento. Sincronizar los procesos, distribuir los datos para aumentar el paralelismo y reducir el \textit{overhead} (coste adicional asociado a la gestión de los recursos y la comunicación entre procesos), son los desafíos recurrentes en los sistemas distribuidos. La comunicación tiene que ser controlada para el correcto funcionamiento y, en algunas ocasiones, estas mejoras derivan en implementaciones más complejas.
	
	Generalmente, en estos casos, es desafiante controlar las acciones de cada proceso para obtener la especificación deseada. Las empresas tecnológicas buscan mejorar el rendimiento y reducir costes de sus sistemas. Para ello, es necesario un uso eficiente de los recursos computacionales. Además, existe un interés en reducir el consumo de energía y las emisiones de CO\(_2\), al entrenar o procesar modelos de IA, puesto que el impacto ambiental está en auge hoy en día. Contando con una gran competitividad en el mercado tecnológico, cualquier mejora, aunque excesivamente significativa, es un avance.


\section{Objetivo}

	El objetivo principal de este trabajo es \textbf{paralelizar algoritmos de IA, empleando técnicas de HPC para reducir el tiempo de ejecución}. Entre los algoritmos a optimizar se encuentran técnicas como el agrupamiento de individuos, predicción de resultados y la optimización de funciones de evaluación. Todas estas implementaciones han sido desarrolladas en \textit{Python}, el lenguaje de programación más popular -actualmente- en el ámbito de la inteligencia artificial\cite{sainin2021best}. Sin embargo, al ser un lenguaje interpretado (el código se traduce en la misma ejecución), aumenta la sobrecarga y hace que -generalmente- el programa sea más lento que la misma implementación en otros lenguajes. Por estos motivos, \textit{Python} es el lenguaje de programación idóneo para aplicar técnicas de cómputo de alto rendimiento y reducir el tiempo de ejecución.
	
	Asimismo, se requerirá alcanzar los siguientes objetivos secundarios:
	
	\begin{enumerate}
		\item \textbf{Diseño de implementaciones escalables y flexibles.} Al diseñar e implementar mejoras de cada algoritmo, es posible utilizar -entre otros parámetros- distintos números de procesos en la ejecución. Esto permite un estudio profundo de las implementaciones realizadas. Al variar el número de procesos se puede comprobar cuál es el número idóneo para cualquier implementación. La flexibilidad en las mejoras permite variar los datos de entrada para que funcione correctamente con un tamaño de \textit{dataset} variable.
		\item \textbf{Correcto funcionamiento de los algoritmos.} Al realizar las mejoras, además de mejorar el rendimiento, es necesario tener una cohesión con el algoritmo original. Es decir, si queremos maximizar una función de evaluación, la implementación de la mejora tiene que proporcionar resultados parecidos o mejores. No resulta útil implementar una mejora que reduzca el tiempo de ejecución, pero que obtenga peores resultados que el algoritmo ejecutado secuencialmente.
		\item \textbf{Estudio empírico.} Se realizará una evaluación de las mejoras para calcular el aumento del rendimiento. Primero, se mide el tiempo de ejecución de cada algoritmo sin mejoras y, posteriormente, se analizan las diferentes implementaciones desarrolladas variando el número de procesos ejecutados. Para finalizar, se prueban las mejores implementaciones de cada algoritmo en un ordenador personal y en un sistema distribuido con \textit{128} núcleos.
	\end{enumerate}





\section{Estructura del documento}
	El resto de este documento está organizado en los siguientes capítulos:
	
	
	\begin{itemize}
		\item Capítulo \hyperref[cap:c2_context]{2}: Contextualización. En este capítulo se proporciona información de cada algoritmo estudiado.
		\item Capítulo \hyperref[cap:c3_implementaciones]{3}: Diseño e implementaciones. Este capítulo describe en detalle las implementaciones desarrolladas para las diferentes técnicas abordadas.
		\item Capítulo \hyperref[cap:c4_estudio]{4}: Estudio empírico, presenta el proceso experimental realizado, el cual consiste en analizar las mejoras propuestas, variando el número de procesos y el tamaño de los datos, en dos entornos distintos, ordenador personal y sistema distribuido.
		\item Capítulo \hyperref[cap:c5_conclu]{5}: Conclusiones y trabajo a futuro. El último capítulo finaliza el trabajo con una síntesis  de los resultados obtenidos, y presenta la proyección del trabajo a futuro.
	\end{itemize}

















