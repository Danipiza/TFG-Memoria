\chapter{Introducción}

En este capítulo se presenta una perspectiva general del contexto en el que se ha llevado a cabo el proyecto. Además de los desafíos enfrentados durante su desarrollo para alcanzar las contribuciones mencionadas, se detallan cada uno de los propósitos perseguidos en él.


\section{Definición y alcance del proyecto}

El desarrollo de las Inteligencias Artificiales en nuestra sociedad ha sido un fenómeno de gran relevancia, además de popular, en los últimos años. Estas tecnologías han llegado para quedarse. Están mejorando nuestra calidad de vida, desde la automatización de tareas hasta la asistencia virtual [20], estas IAs desempeñan un papel cada vez más importante en nuestro día a día.
Con el advenimiento del Internet de alta velocidad y la proliferación de datos, las empresas tecnológicas se enfrentan a la necesidad creciente de desarrollar servicios de alta calidad en un mercado muy competitivo. Actualmente, se invierte mucho dinero y tiempo en mejorar y diseñar algoritmos, para implementar Inteligencias Artificiales para el acceso público[1].

El entrenamiento y ejecución de estos algoritmos para modelar inteligencias artificiales consumen mucha energía, además de provocar una cantidad excesiva de emisiones de CO2. La empresa tecnológica \textit{Hugging Face} estimó que el entrenamiento de \textit{BLOOM}[16] emitió 25 toneladas de CO2, cifra que se duplicó al contar el coste de producción del equipo informático usado[14]. Los investigadores se están enfocando en evaluar y reducir el impacto ambiental de las tecnologías de IA. Una prueba de ello es el desarrollo de \textit{CarbonTracker}[17] (CTI), un equipo de especialistas financieros que asumen el riesgo climático como realidad de los mercados financieros actuales. El objetivo de esta herramienta es predecir y reducir la huella de carbono de las etapas de entrenamiento de los modelos de IA[18].

El uso de la programación distribuida, más específicamente, aplicaciones basadas en MPI, permite tener varios procesos ejecutándose en paralelo, dividiendo la carga de trabajo y así reduciendo el tiempo de ejecución. Al contrario de la memoria compartida, en la cual se pueden dar problemas de memoria y sincronización, cada proceso generado tiene su propia memoria local, evadiendo estos problemas. Sin embargo, hay que diseñar implementaciones correctas y eficientes para no tener más carga espacial de la esperada. Las conexiones entre los procesos se pueden moldear para maximizar la eficiencia y reducir el tiempo de cómputo. Una de las más populares es el modelo master-worker. El proceso master se encarga de distribuir el trabajo a los workers, para, en paralelo, ejecutar la misma tarea pero con conjuntos mucho más reducidos. Al finalizar la tarea, el worker envía su salida, y si el proceso master no ha terminado, espera para recibir más datos. MPI permite la comunicación eficiente entre procesos, mejorando la escalabilidad y reduciendo el tiempo de procesamiento. Esta metodología, además de mejorar el rendimiento, también simplifica la gestión de recursos, mejorando la utilización del hardware disponible en el sistema. 

Los algoritmos de IA suelen manejar un vasto número de datos para entrenar y evaluar los modelos deseados. Por eso es fundamental diseñar estrategias para distribuir los datos, y dividir las cargas de trabajo de manera equitativa, controlando el flujo de datos para evitar cuellos de botella, y equilibrar los recursos disponibles. Esto incluye, además de la optimización del tiempo de ejecución, una gestión efectiva de la memoria (complejidad espacial) y minimizar la latencia (tiempo de espera en transmitir los paquetes de información en una red) en la comunicación entre los procesos.
En este proyecto se diseñarán e implementarán varias optimizaciones para diferentes algoritmos de IA, con el objetivo de reducir el tiempo de ejecución.





\section{Motivación}
Actualmente hay muchas implementaciones de algoritmos de IA. Scikit learn es una biblioteca de Python perfecta para probar cualquier técnica. Esta biblioteca, como la mayoría, ejecuta los algoritmos de manera secuencial, sin dividir la carga de trabajo.  Las implementaciones están estudiadas y perfeccionadas para realizar los cálculos en un único proceso, pero se puede reducir el tiempo de ejecución aplicando paralelismo. 

Las arquitecturas distribuidas junto con las técnicas de cómputo de alto rendimiento (HPC, por sus siglas en inglés), son una de las soluciones más apropiadas para los usuarios finales: científicos y empresas. La búsqueda de un rendimiento óptimo, bajo una perspectiva técnica en sistemas altamente distribuidos, demandan enfoques adaptables y escalables para implementar aplicaciones científicas de alto rendimiento. Sincronizar los procesos, distribuir los datos para aumentar el paralelismo y reducir el overhead, son los desafíos recurrentes en los sistemas distribuidos. La comunicación en el proceso tiene que ser controlada para el correcto funcionamiento, y en algunas ocasiones estas mejoras derivan en implementaciones más complejas.

En estas situaciones, es desafiante controlar las acciones de cada proceso, para obtener la especificación deseada. Cada algoritmo tiene su funcionamiento y su desempeño, al igual que implementación única.

Las empresas tecnológicas buscan mejorar el rendimiento y reducir costos de sus sistemas. Para ello tienen que hacer un uso eficiente de los recursos computacionales. Además de buscar reducir el consumo de energía y las emisiones de CO2, al entrenar o procesar modelos de IA, puesto que el impacto ambiental está en auge hoy en día.  Contando con una gran competitividad en el mercado tecnológico, cualquier mejora, aunque no sea tan significativa, es un avance.




\newpage


\section{Objetivo}

El objetivo principal de este trabajo es \textbf{paralelizar algoritmos de IA, desarrollando varias implementaciones que reduzcan el tiempo de ejecución}. Entre los algoritmos a optimizar se encuentran técnicas como el agrupamiento de individuos, predicción de resultados y la optimización de funciones de evaluación. Todas estas implementaciones han sido desarrolladas en Python, el lenguaje de programación más popular en el ámbito de la inteligencia artificial [21]. Sin embargo, al ser un lenguaje interpretado (el código se traduce en la misma ejecución), aumenta la sobrecarga y hace que el programa sea más lento que la misma implementación en otros lenguajes. Por eso Python es el lenguaje de programación idóneo para aplicar técnicas de cómputo de alto rendimiento y reducir el tiempo de ejecución.

Asimismo, se requerirá alcanzar los siguientes objetivos secundarios:

\begin{enumerate}
	\item \textbf{Diseño de implementaciones escalables y flexibles.} Al diseñar e implementar mejoras de cada algoritmo, se permite la varianza del número de procesos a ejecutar. Esto permite un estudio profundo de las implementaciones realizadas. Al variar el número de procesos se puede comprobar cuál es el número idóneo para cualquier implementación, así calculando el número óptimo de procesos. La flexibilidad en las mejoras permite variar los datos de entrada para que funcione correctamente con un tamaño de dataset variable.
	\item \textbf{Correcto funcionamiento de los algoritmos.} Al realizar las mejoras, además de mejorar el rendimiento, tienen que tener una cohesión con el algoritmo original. Es decir, si queremos maximizar una función de evaluación, la implementación de la mejora tiene que dar resultados parecidos o mejores. No sirve implementar una mejora que reduzca el tiempo de ejecución pero no de buenos resultados.
	\item \textbf{Estudio empírico.} Se realizará una evaluación de las mejoras para calcular el aumento del rendimiento. Primero se mide el tiempo que tarda cada algoritmo sin mejoras y luego las diferentes implementaciones desarrolladas, variando el número de procesos ejecutados. Para finalizar se prueban las mejores implementaciones de cada algoritmo en el sistema distribuido con 128 núcleos.
	\item \textcolor{red}{Cálculo del mejor número de procesos para cada mejora, dado un dataset.}
\end{enumerate}





\section{Estructura del documento}
El resto de este documento está organizado en los siguientes capítulos:


\begin{itemize}
	\item Capítulo \hyperref[cap:c2_context]{2}: Contextualización. En este capítulo se proporciona información de cada algoritmo estudiado, para la correcta lectura del trabajo.
	\item Capítulo \hyperref[cap:c3_implementaciones]{3}: Diseño e implementaciones, Comienza describiendo unos ejemplos básicos fuera del ámbito de la inteligencia artificial, seguido de las implementaciones desarrolladas para las diferentes técnicas abordadas.
	\item Capítulo \hyperref[cap:c4_estudio]{4}: Estudio empírico, presenta el estudio empírico realizado, el cual consiste en medir las mejoras, con los algoritmos secuenciales, variando el número de procesos y el tamaño de los datos. Para poder medir el speed-up y realizar comparaciones significativas.
	\item Capítulo \hyperref[cap:c5_conclu]{5}: Conclusiones y trabajo a futuro.
\end{itemize}
















%The document is divided into \texttt{chapters}, \texttt{sections}, and \texttt{subsections}.
%
%Some important references are \cite{einstein,latexcompanion,knuthwebsite}.
%
%To add paragraphs in the document, 
%one line break is not enough,
%
%two line breaks are needed.
%
%An itemized list:
%
%\begin{itemize}
%	\item An item.
%	\item Another item.
%	\item Final item.
%\end{itemize}
%
%An enumerated list:
%
%\begin{enumerate}
%	\item First item.
%	\item Second item.
%	\item Third item.
%\end{enumerate}
%
%A figure with an image is presented in \Cref{fig:figura}. Note that it floats away and latex places it where convenient.
%
%\begin{figure}[h!] % h de here con signo o sin b abajo t al principio
%	\centering
%	\includegraphics[width=0.4\textwidth]{Images/escudo_ucm.pdf}
%	\caption{Sample figure} % DESCRIPCION
%	\label{fig:figura}
%\end{figure}
%
%
%Tables work in the same way, as seen in \Cref{tab:tabla}
%
%\begin{table}
%	\centering
%	\begin{tabular}{c|c|c}
%		Row & English & Español \\\hline\hline
%		1 & One & Uno \\
%		2 & Two & Dos \\
%	\end{tabular}
%	\caption{Sample table}
%	\label{tab:tabla}
%\end{table}
%
%
%
%TODO QUITAR TABS \usepackage{parskip}
%
%\lstset{language=python, breaklines=true, basicstyle=\footnotesize}
%\begin{lstlisting}[frame=single]
%	# Prueba
%	print("Hola Mundo\n")
%	
%\end{lstlisting}
%
%-_-
%-\_-
%\%
%\#
%@
%º
















